{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a9efd5",
   "metadata": {
    "papermill": {
     "duration": 0.003089,
     "end_time": "2024-06-24T15:41:44.328878",
     "exception": false,
     "start_time": "2024-06-24T15:41:44.325789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79edb45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:41:44.335606Z",
     "iopub.status.busy": "2024-06-24T15:41:44.335264Z",
     "iopub.status.idle": "2024-06-24T15:41:58.649471Z",
     "shell.execute_reply": "2024-06-24T15:41:58.648517Z"
    },
    "papermill": {
     "duration": 14.319841,
     "end_time": "2024-06-24T15:41:58.651460",
     "exception": false,
     "start_time": "2024-06-24T15:41:44.331619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv -n /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "echo \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bfdb17f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:41:58.659312Z",
     "iopub.status.busy": "2024-06-24T15:41:58.659007Z",
     "iopub.status.idle": "2024-06-24T15:41:58.669366Z",
     "shell.execute_reply": "2024-06-24T15:41:58.668364Z"
    },
    "papermill": {
     "duration": 0.0166,
     "end_time": "2024-06-24T15:41:58.671235",
     "exception": false,
     "start_time": "2024-06-24T15:41:58.654635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/main.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    \n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "import re\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "                                                    \n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Load the model           \n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "    \n",
    "### Utils function\n",
    "def extract_bold_text(text):\n",
    "  pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n",
    "  matches = re.findall(pattern, text)\n",
    "  to_ret = matches[0] if len(matches) > 0 else text\n",
    "  parts = to_ret.split(\":\") \n",
    "  to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n",
    "  return to_ret\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, variant, device_str):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device_str)\n",
    "        self._weigts_path = WEIGHTS_PATH\n",
    "\n",
    "        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "          self._model = GemmaForCausalLM(model_config)\n",
    "          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n",
    "          self._model.load_weights(ckpt_path)\n",
    "          self._model = self._model.to(self._device).eval()\n",
    "            \n",
    "\n",
    "    def get_response_from_llm(self, obs, text): \n",
    "        CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        prompts = CHAT_TEMPLATE.format(prompts=text)\n",
    "        \n",
    "        try:\n",
    "            resp = self._model.generate(prompts, device=self._device, output_len=150, **sampler_kwargs)\n",
    "            print(f\"\"\"prompts: {prompts}\\nresp: {resp}\"\"\")\n",
    "        except RuntimeError as e:\n",
    "            resp = \"**no**\"\n",
    "        \n",
    "        resp = resp.replace(\"**Sure, here's the question:**\", \"\")\n",
    "        resp = resp.replace(\"Sure, here's the conversion:\", \"\")\n",
    "        resp = resp.replace(\"Sure, \", \"\")\n",
    "        resp = resp.replace(\"here's the \", \"\")\n",
    "        resp = resp.replace(\"question:\", \"\")\n",
    "        resp = resp.replace(\"conversion:\", \"\")\n",
    "        resp = resp.replace(\"**Guess:**\", \"\")\n",
    "        resp = resp.replace(\"**Anwser:**\", \"\")\n",
    "        resp = resp.replace(\"**Question:**\", \"\")\n",
    "\n",
    "        return extract_bold_text(resp)\n",
    "\n",
    "class Questioner(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = \"\"\"You are a AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question to identify the secret, and \"guesses\" to identify the secret word. The answerer reply only \"yes or \"no\". The secret could be a person, a place / geographic location, or a physical object in the world.\"\"\"\n",
    "\n",
    "        if obs.turnType == 'guess':\n",
    "            if obs.step > 0:\n",
    "                self._information = f\"\"\"Previous question: {obs.questions[-1]}. The anwser is: {obs.answers[-1]}\"\"\"\n",
    "#                 self._information = super().get_response_from_llm(obs,\n",
    "#                     f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\". Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\"\"\"\n",
    "#                 )\n",
    "            prompts += f\"\"\"\n",
    "Now, you are the guesser. Guess for the keyword.\n",
    "Note:\n",
    "- The guess \"MUST\" be the keyword only. DO NOT provider the reason.\n",
    "- Example \"**Grand Palace**\".\n",
    "- The guess must less than 100 charactors. Wrapped the question within **.\n",
    "- {self._information}\n",
    "\"\"\"\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "\n",
    "        elif obs.turnType == 'ask':\n",
    "            prompts += f\"\"\"\n",
    "Now, you are the questioner. It is best to ask questions that are as broad as possible. The goal of the questions should be to identify what category the object in question is. All questions asked should pertain to ascertaining the category of the item.\n",
    "Note:\n",
    "-The question must not have any text before or after.\n",
    "-The question must less than 100 charactors. Wrapped the question within **.\n",
    "\"\"\"\n",
    "                       \n",
    "            if obs.step == 0:\n",
    "                prompts += \"\"\"\n",
    "BEGIN EXAMPLE\n",
    " - Is it a man-made?\n",
    " - Is its size bigger than a house?\n",
    " - Is it used in the kitchen?\n",
    " - Is it in?\n",
    "END EXAMPLE\"\"\"\n",
    "\n",
    "            else:\n",
    "                prompts += f\"\"\"Note: {self._information}\"\"\"\n",
    "\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "\n",
    "class Answerer(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answerer reply only \"yes or \"no\". The keyword is a thing.\n",
    "        You are the answerer, Yes or no if the question: \\\"{obs.questions[-1]}\\\", is the right answer for \\\"{obs.keyword}\\\".\n",
    "        Note:\n",
    " -Answer only yes or no.\n",
    " -Example: no\n",
    " -Wrapped the answer within **.\"\"\"\n",
    "        \n",
    "        response =  super().get_response_from_llm(obs, prompts)\n",
    "       \n",
    "        if response == None:\n",
    "            response = \"no\"\n",
    "\n",
    "        response = response.replace(\".\", \"\")\n",
    "        if response not in [\"yes\", \"no\"]: \n",
    "            response = \"no\"\n",
    "        \n",
    "        return response\n",
    "                \n",
    "############################################################\n",
    "agent = None\n",
    "\n",
    "VARIANT = \"7b-it-quant\"\n",
    "# VARIANT = \"2b-it\"\n",
    "DEVICE = \"cuda\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "# DEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_agent(name):\n",
    "    global agent\n",
    "\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Questioner(variant=VARIANT, device_str=DEVICE)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Answerer(variant=VARIANT, device_str=DEVICE)\n",
    "\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    print(f\"\"\"--obsinfo: {obs}\"\"\")\n",
    "    selected_agent = None\n",
    "    if obs.turnType == \"ask\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    elif obs.turnType == \"guess\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    else:\n",
    "        selected_agent = get_agent(\"answerer\")\n",
    "        \n",
    "    try:\n",
    "        response = selected_agent.call(obs)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        response = \"no\"\n",
    "\n",
    "    if response is None or len(response) <= 1: return \"no\" \n",
    "    else: return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc81cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:41:58.678164Z",
     "iopub.status.busy": "2024-06-24T15:41:58.677898Z",
     "iopub.status.idle": "2024-06-24T15:41:58.683117Z",
     "shell.execute_reply": "2024-06-24T15:41:58.682410Z"
    },
    "papermill": {
     "duration": 0.010728,
     "end_time": "2024-06-24T15:41:58.684927",
     "exception": false,
     "start_time": "2024-06-24T15:41:58.674199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %run submission/main.py\n",
    "\n",
    "# def agent_fn(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# debug_config = {'episodeSteps':20,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 120,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "# from kaggle_environments import make\n",
    "# # env = make(\"llm_20_questions\", debug=True, configuration=debug_config)\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# agent = \"/kaggle/working/submission/main.py\"\n",
    "# env.reset()\n",
    "# logs = env.run([agent, agent_fn, agent_fn, agent_fn])\n",
    "# #while not env.done: #add steps here for testing\n",
    "# env.render(mode=\"ipython\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d748f021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:41:58.692061Z",
     "iopub.status.busy": "2024-06-24T15:41:58.691803Z",
     "iopub.status.idle": "2024-06-24T15:41:58.695905Z",
     "shell.execute_reply": "2024-06-24T15:41:58.695042Z"
    },
    "papermill": {
     "duration": 0.00989,
     "end_time": "2024-06-24T15:41:58.697878",
     "exception": false,
     "start_time": "2024-06-24T15:41:58.687988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# # For debugging, play game with only two rounds\n",
    "# debug_config = {'episodeSteps': 31,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 60,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "\n",
    "# env = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "# print(\"start.....\")\n",
    "# game_output = env.run(agents=[agent_fn, agent_fn, agent_fn, agent_fn])\n",
    "# print(\"finish....\")\n",
    "# env.render(mode=\"ipython\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389a175b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:41:58.705066Z",
     "iopub.status.busy": "2024-06-24T15:41:58.704814Z",
     "iopub.status.idle": "2024-06-24T15:42:05.819835Z",
     "shell.execute_reply": "2024-06-24T15:42:05.818674Z"
    },
    "papermill": {
     "duration": 7.121261,
     "end_time": "2024-06-24T15:42:05.822371",
     "exception": false,
     "start_time": "2024-06-24T15:41:58.701110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a95bac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T15:42:05.830806Z",
     "iopub.status.busy": "2024-06-24T15:42:05.830494Z",
     "iopub.status.idle": "2024-06-24T15:44:18.655222Z",
     "shell.execute_reply": "2024-06-24T15:44:18.654262Z"
    },
    "papermill": {
     "duration": 132.831538,
     "end_time": "2024-06-24T15:44:18.657464",
     "exception": false,
     "start_time": "2024-06-24T15:42:05.825926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 157.365922,
   "end_time": "2024-06-24T15:44:18.989459",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-24T15:41:41.623537",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
