{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"},{"sourceId":11358,"sourceType":"modelInstanceVersion","modelInstanceId":5383},{"sourceId":11359,"sourceType":"modelInstanceVersion","modelInstanceId":8749}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":169.923583,"end_time":"2024-04-17T13:50:23.369773","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-17T13:47:33.44619","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sanhapon/test-fork-of-llm-20-questions-simple-version?scriptVersionId=183641225\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. ","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working\npip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\ngit clone https://github.com/google/gemma_pytorch.git > /dev/null\nmkdir -p /kaggle/working/submission/lib/gemma/\nmv -n /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\necho \"done\"","metadata":{"papermill":{"duration":13.257308,"end_time":"2024-04-17T13:47:49.310664","exception":false,"start_time":"2024-04-17T13:47:36.053356","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-15T05:57:47.917266Z","iopub.execute_input":"2024-06-15T05:57:47.917998Z","iopub.status.idle":"2024-06-15T05:58:01.169366Z","shell.execute_reply.started":"2024-06-15T05:57:47.917965Z","shell.execute_reply":"2024-06-15T05:58:01.168553Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Cloning into 'gemma_pytorch'...\n","output_type":"stream"},{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"# %%writefile submission/main.py?\n# Setup\nimport os\nimport sys\nimport gc\n\nsys.path.insert(0, \"/kaggle/working/submission/lib/\")\nWEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n\nimport contextlib\nfrom pathlib import Path\n\nimport torch\nfrom gemma.config import get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nimport re\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n\n# Load the model           \n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n    \n\nmodel_config =  get_config_for_7b()\nmodel_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\nmodel_config.quant = True\n\ndevice = torch.device(\"cuda\")\nmodel = None\ngc.collect()\n\nprint(f\"create model... {model_config}\\r\\n\")\nwith _set_default_tensor_type(model_config.get_dtype()):\n    model = GemmaForCausalLM(model_config)\n    ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-7b-it-quant.ckpt')\n    model.load_weights(ckpt_path)\n    model = model.to(device).eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:08:44.876647Z","iopub.execute_input":"2024-06-15T06:08:44.877444Z","iopub.status.idle":"2024-06-15T06:09:47.037353Z","shell.execute_reply.started":"2024-06-15T06:08:44.877412Z","shell.execute_reply":"2024-06-15T06:09:47.036299Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"create model... GemmaConfig(vocab_size=256000, max_position_embeddings=8192, num_hidden_layers=28, num_attention_heads=16, num_key_value_heads=16, hidden_size=3072, intermediate_size=24576, head_dim=256, rms_norm_eps=1e-06, dtype='bfloat16', quant=True, tokenizer='/kaggle/input/gemma/pytorch/7b-it-quant/2/tokenizer.model')\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_response(prompts):\n    CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}.<end_of_turn>\\n<start_of_turn>model\\n\"\n    sampler_kwargs = {\n        'temperature': 0.01,\n        'top_p': 0.1,\n        'top_k': 1,\n    }\n    resp = model.generate(\n                    CHAT_TEMPLATE.format(prompts=prompts),\n                    device=device,\n                    output_len=100,\n                    **sampler_kwargs\n    )\n\n    return resp\n\nprompts = [\n         \nf\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the yes-or-no-question and predict the secret. The secret is the place.\nThe sample question: Is it the country in Asia? Now, you are the questioner, only state your yes-or-no question without conversational sentence to find the place.\nWrap your question within **.\n\"\"\", \n]\n\nfor i, p in enumerate(prompts):\n    resp = get_response(p)\n    print(f\"\"\"{i}: {p} \\r\\n{resp}\\r\\n------------------------------\"\"\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-15T06:43:30.442712Z","iopub.execute_input":"2024-06-15T06:43:30.443071Z","iopub.status.idle":"2024-06-15T06:46:16.227686Z","shell.execute_reply.started":"2024-06-15T06:43:30.443036Z","shell.execute_reply":"2024-06-15T06:46:16.226781Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"0: \"We are playing the 20 Questions game. The questioner's goal is to ask the yes-or-no-question and predict the secret. The secret is the place.\nThe sample question: Is it the country in Asia? Now, you are the questioner, only state your yes-or-no question without conversational sentence to find the place.\nWrap your question within **.\n \n**Is the secret place a country in Europe?**\n\nThe questioner's goal is to ask a yes-or-no question and predict the secret place, which is a country in Asia.\n------------------------------\n1: \"We are playing the 20 Questions game. The questioner's goal is to ask the yes-or-no-question and predict the place.\nThe sample question: Is it the country in Asia? Now, you are the questioner, only state your yes-or-no question without conversational sentence to find the place.\nWrap your question within **.\n \n**Is the country of Italy located in Europe?**\n\nThe questioner's goal is to ask a yes-or-no question and predict the place.\n------------------------------\n2: \"We are playing the 20 Questions game. The questioner's goal is to ask the yes-or-no-question and predict the place.\nThe sample question: Is it the country in Asia? Now, you are the questioner, only state yes-or-no question only. Wrap your question within **.\n \n**Sure, here's your question:**\n\n**Is the country of Australia located in Asia?**\n------------------------------\n3: \"We are playing the 20 Questions game. The questioner's goal is to ask the yes-or-no-question and predict the place.\nThe sample question: Is it the country in Asia? Now, you are the questioner, only state yes-or-no question only, do not explain. Wrap your question within **.\n \n**Is the continent of Africa located in Asia?**\n\nThe questioner's goal is to ask a yes-or-no question and predict the place.\n------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"prompts = [\nf\"\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name.\nThe sample question: Is it a Asia continent? Now, you are the questioner, only state your yes-or-no question without conversational sentence to find the place.\"\"\", \n         \nf\"\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name.\nThe sample question: Is it a Asia continent? Now, you are the questioner, only state your yes-or-no question to find the place.\"\"\"\n]\n\n----\n\n\n0: **Question:** Is the secret place a continent or a city?\n\n**The secret is:** Asia.\n1: **Sure, here's your question:**\n\n**Is the secret place a continent located in the east of the world?**","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile submission/main.py?\n# Setup\nimport os\nimport sys\n\n# **IMPORTANT:** Set up your system path like this to make your code work\n# both in notebooks and in the simulations environment.\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n    WEIGHTS_PATH  = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\nelse:\n    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n    WEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n#     WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n\n\nimport contextlib\nfrom pathlib import Path\n\nimport torch\nfrom gemma.config import get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nimport re\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n\n# Load the model           \n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n    \n### Utils function\ndef extract_bold_text(text):\n  pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n  matches = re.findall(pattern, text)\n  to_ret = matches[0] if len(matches) > 0 else text\n  parts = to_ret.split(\":\") \n  to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n  return to_ret\n\nclass BaseAgent:\n    def __init__(self, variant, device_str):\n        self._variant = variant\n        self._device = torch.device(device_str)\n        self._weigts_path = WEIGHTS_PATH\n\n        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n        model_config.quant = \"quant\" in variant\n\n        print(f\"create model... {model_config}\\r\\n\")\n        with _set_default_tensor_type(model_config.get_dtype()):\n          self._model = GemmaForCausalLM(model_config)\n          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n          self._model.load_weights(ckpt_path)\n          self._model = self._model.to(self._device).eval()\n\n    def get_response_from_llm(self, obs, prompts): \n        CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}. Wrap your though with **.<end_of_turn>\\n<start_of_turn>model\\n\"\n\n        sampler_kwargs = {\n            'temperature': 0.01,\n            'top_p': 0.1,\n            'top_k': 1,\n        }\n        \n        try:\n            resp = self._model.generate(\n                CHAT_TEMPLATE.format(prompts=prompts),\n                device=self._device,\n                output_len=150,\n                **sampler_kwargs\n            )\n            print(f\"\"\"prompts: {prompts}\\nresp: {resp}\"\"\")\n        except RuntimeError as e:\n            print(f\"Error in LLM generation: {e}\")\n            resp = \"**no**\"\n            resp = resp.replace(\"**Question:**\", \"\").replace(\"**Guess:**\", \"\").replace(\"**Anwser:**\", \"\")\n        return extract_bold_text(resp)\n\nclass Questioner(BaseAgent):\n    def __init__(self, variant, device_str):\n        super().__init__(variant, device_str)\n        self._information = \"\"\n    \n    def call(self, obs):\n        prompts = f\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name.\"\"\"\n        if obs.turnType == 'guess':\n            self._information = super().get_response_from_llm(obs,\n                f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\". Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\"\"\"\n            )\n            prompts = f\"\"\"Based on this information \\\"{self._information}\\\". Guess the place based on above information.\"\"\"\n            return super().get_response_from_llm(obs, prompts)\n\n        elif obs.turnType == 'ask':\n            if obs.step == 0:\n                prompts += f\"\"\"The sample question: Is it a Asia continent? Now, you are the questioner, generate a yes-or-no question to find the place. \"\"\"\n                return super().get_response_from_llm(obs, prompts)\n            else:\n                prompts = f\"\"\"We know this information \\\"{self._information}\\\". Now, you are the questioner, based on known information, generate yes-or-no question to find \"specific\" place.\"\"\"\n\n                return super().get_response_from_llm(obs, prompts)\n                \n\nclass Answerer(BaseAgent):\n    def __init__(self, variant, device_str):\n        super().__init__(variant, device_str)\n        self._information = \"\"\n    \n    def call(self, obs):\n        prompts = f\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name. If this question \\\"{obs.questions[-1]}\\\" relates to \\\"{obs.keyword}\\\" return \\\"yes\\\" otherwise return \\\"no\\\".\n                    \"\"\"\n        \n        return super().get_response_from_llm(obs, prompts)\n                \n############################################################\nagent = None\n\nVARIANT = \"7b-it-quant\"\n# VARIANT = \"2b-it\"\n# DEVICE = \"cuda\"\nDEVICE = \"cpu\"\n\n# DEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ndef get_agent(name):\n    global agent\n\n    if agent is None and name == 'questioner':\n        agent = Questioner(variant=VARIANT, device_str=DEVICE)\n    elif agent is None and name == 'answerer':\n        agent = Answerer(variant=VARIANT, device_str=DEVICE)\n\n    assert agent is not None, \"Agent not initialized.\"\n    return agent\n\n\ndef agent_fn(obs, cfg):\n    selected_agent = None\n    if obs.turnType == \"ask\":\n        selected_agent = get_agent(\"questioner\")\n    elif obs.turnType == \"guess\":\n        selected_agent = get_agent(\"questioner\")\n    else:\n        selected_agent = get_agent(\"answerer\")\n        \n    try:\n        response = selected_agent.call(obs)\n    except Exception as e:\n        print(f\"error: {e}\")\n        response = \"no\"\n    \n\n    if obs.turnType == \"answer\":\n        response = response.replace(\".\", \"\")\n        if response not in [\"yes\", \"no\"]: \n            response = \"no\"\n\n    if response is None or len(response) <= 1: return \"no\" \n    else: return response","metadata":{"execution":{"iopub.status.busy":"2024-06-15T04:49:17.879332Z","iopub.execute_input":"2024-06-15T04:49:17.879743Z","iopub.status.idle":"2024-06-15T04:49:21.440714Z","shell.execute_reply.started":"2024-06-15T04:49:17.879685Z","shell.execute_reply":"2024-06-15T04:49:21.439537Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config_for_7b, get_config_for_2b\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgemma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GemmaForCausalLM\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gemma'"],"ename":"ModuleNotFoundError","evalue":"No module named 'gemma'","output_type":"error"}]},{"cell_type":"code","source":"def simple_agent(obs, cfg):\n    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n    elif obs.turnType == \"guess\": response = \"pig\"\n    elif obs.turnType == \"answer\": response = \"yes\"\n    return response\n\nfrom kaggle_environments import make\n# For debugging, play game with only two rounds\ndebug_config = {'episodeSteps': 10,     # initial step plus 3 steps per round (ask/answer/guess)\n                'actTimeout': 60,       # agent time per round in seconds; default is 60\n                'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n                'agentTimeout': 3600}  # obsolete field; default is 3600\n\nenv = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n\nprint(\"start.....\")\ngame_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\nprint(\"finish....\")\nenv.render(mode=\"ipython\", width=700, height=700)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !apt install pigz pv > /dev/null","metadata":{"papermill":{"duration":5.560311,"end_time":"2024-04-17T13:47:54.892856","exception":false,"start_time":"2024-04-17T13:47:49.332545","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2","metadata":{"papermill":{"duration":148.240766,"end_time":"2024-04-17T13:50:23.136669","exception":false,"start_time":"2024-04-17T13:47:54.895903","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}