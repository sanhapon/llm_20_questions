{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7410c0ae",
   "metadata": {
    "papermill": {
     "duration": 0.0035,
     "end_time": "2024-06-16T16:07:34.260764",
     "exception": false,
     "start_time": "2024-06-16T16:07:34.257264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc68f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:34.268231Z",
     "iopub.status.busy": "2024-06-16T16:07:34.267902Z",
     "iopub.status.idle": "2024-06-16T16:07:48.891758Z",
     "shell.execute_reply": "2024-06-16T16:07:48.890884Z"
    },
    "papermill": {
     "duration": 14.630142,
     "end_time": "2024-06-16T16:07:48.894010",
     "exception": false,
     "start_time": "2024-06-16T16:07:34.263868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv -n /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "echo \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde6f2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:48.902081Z",
     "iopub.status.busy": "2024-06-16T16:07:48.901496Z",
     "iopub.status.idle": "2024-06-16T16:07:48.907561Z",
     "shell.execute_reply": "2024-06-16T16:07:48.906716Z"
    },
    "papermill": {
     "duration": 0.01208,
     "end_time": "2024-06-16T16:07:48.909467",
     "exception": false,
     "start_time": "2024-06-16T16:07:48.897387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import gc\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/working/submission/lib/\")\n",
    "# WEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# import contextlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# import torch\n",
    "# from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "# from gemma.model import GemmaForCausalLM\n",
    "# import re\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# # Load the model           \n",
    "# @contextlib.contextmanager\n",
    "# def _set_default_tensor_type(dtype: torch.dtype):\n",
    "#   torch.set_default_dtype(dtype)\n",
    "#   yield\n",
    "#   torch.set_default_dtype(torch.float)\n",
    "    \n",
    "\n",
    "# model_config =  get_config_for_7b()\n",
    "# model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "# model_config.quant = True\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# model = None\n",
    "# gc.collect()\n",
    "\n",
    "# print(f\"create model... {model_config}\\r\\n\")\n",
    "# with _set_default_tensor_type(model_config.get_dtype()):\n",
    "#     model = GemmaForCausalLM(model_config)\n",
    "#     ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-7b-it-quant.ckpt')\n",
    "#     model.load_weights(ckpt_path)\n",
    "#     model = model.to(device).eval()\n",
    "\n",
    "    \n",
    "# def get_response(prompts):\n",
    "#     CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}.<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "#     sampler_kwargs = {\n",
    "#         'temperature': 0.01,\n",
    "#         'top_p': 0.1,\n",
    "#         'top_k': 1,\n",
    "#     }\n",
    "#     resp = model.generate(\n",
    "#                     CHAT_TEMPLATE.format(prompts=prompts),\n",
    "#                     device=device,\n",
    "#                     output_len=100,\n",
    "#                     **sampler_kwargs\n",
    "#     )\n",
    "\n",
    "#     return resp\n",
    "\n",
    "# prompts = [\n",
    "#         \"\"\"We are playing the 20 Questions game. The questioner asks the yes-or-no-question and guesses to identify the secret word . The keyword is the place. The answerer replies yes-or-no only.\n",
    "# Ask yes-or-no question without conversational sentence. Without information, your question must check is this place located in which continent.\n",
    "# Your generated text must less than 100 charactors and wrapped within **.\"\"\",\n",
    "    \n",
    "#     \"\"\"We are playing the 20 Questions game. The questioner asks the yes-or-no-question and guesses to identify the secret word . The keyword is the place. The answerer replies yes-or-no only.\n",
    "#           Based on this information: \\\"It is in the North America continent\\\"\n",
    "#           As a questioner, generate yes-no question to narrow down to city or area.\n",
    "#           The text must less than 30 charactors and wrapped within **.\"\"\"\n",
    "# ]\n",
    "\n",
    "# for i, p in enumerate(prompts):\n",
    "#     resp = get_response(p)\n",
    "#     print(f\"\"\"{i}: {p} \\r\\n{resp}\\r\\n------------------------------\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9f7d77a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:48.917211Z",
     "iopub.status.busy": "2024-06-16T16:07:48.916976Z",
     "iopub.status.idle": "2024-06-16T16:07:48.926849Z",
     "shell.execute_reply": "2024-06-16T16:07:48.926031Z"
    },
    "papermill": {
     "duration": 0.016069,
     "end_time": "2024-06-16T16:07:48.928731",
     "exception": false,
     "start_time": "2024-06-16T16:07:48.912662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "#     WEIGHTS_PATH  = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "#     WEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "import re\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Load the model           \n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "    \n",
    "### Utils function\n",
    "def extract_bold_text(text):\n",
    "  pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n",
    "  matches = re.findall(pattern, text)\n",
    "  to_ret = matches[0] if len(matches) > 0 else text\n",
    "  parts = to_ret.split(\":\") \n",
    "  to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n",
    "  return to_ret\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, variant, device_str):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device_str)\n",
    "        self._weigts_path = WEIGHTS_PATH\n",
    "\n",
    "        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "          self._model = GemmaForCausalLM(model_config)\n",
    "          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n",
    "          self._model.load_weights(ckpt_path)\n",
    "          self._model = self._model.to(self._device).eval()\n",
    "\n",
    "    def get_response_from_llm(self, obs, text): \n",
    "        CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        prompts = CHAT_TEMPLATE.format(prompts=text)\n",
    "        \n",
    "        try:\n",
    "            resp = self._model.generate(\n",
    "                prompts,\n",
    "                device=self._device,\n",
    "                output_len=150,\n",
    "                **sampler_kwargs\n",
    "            )\n",
    "            print(f\"\"\"prompts: {prompts}\\nresp: {resp}\\r\\n-------------step end--------------\"\"\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error in LLM generation: {e}\")\n",
    "            resp = \"**no**\"\n",
    "        \n",
    "        resp = resp.replace(\"**Sure, here's the question:**\", \"\")\n",
    "        resp = resp.replace(\"Sure, here's the conversion:\", \"\")\n",
    "        resp = resp.replace(\"Sure, \", \"\")\n",
    "        resp = resp.replace(\"here's the \", \"\")\n",
    "        resp = resp.replace(\"question:\", \"\")\n",
    "        resp = resp.replace(\"conversion:\", \"\")\n",
    "        resp = resp.replace(\"**Guess:**\", \"\")\n",
    "        resp = resp.replace(\"**Anwser:**\", \"\")\n",
    "        resp = resp.replace(\"**Question:**\", \"\")\n",
    "\n",
    "        return extract_bold_text(resp)\n",
    "\n",
    "class Questioner(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answere reply only \"yes or \"no\". The keyword is the place.\"\"\"\n",
    "\n",
    "        if obs.turnType == 'guess':\n",
    "            if obs.step > 0:\n",
    "                self._information = super().get_response_from_llm(obs,\n",
    "                    f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\". Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\"\"\"\n",
    "                )\n",
    "            prompts += f\"\"\"Now, you are the guesser. Guess for the keyword.\n",
    "                        \\rNote:\n",
    "                        \\r- The guess \"MUST\" be the keyword only. DO NOT provider the reason.\n",
    "                        \\r- Example \"**Grand Palace**\".\n",
    "                        \\r- The guess must less than 100 charactors. Wrapped the question within **.\n",
    "                        \\r - {self._information}\n",
    "                        \"\"\"\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "\n",
    "        elif obs.turnType == 'ask':\n",
    "            prompts += f\"\"\"Now, you are the questioner. Ask yes-or-no question without conversational sentence. Your question must check and narrow the place is located in somewhere.\n",
    "                        \\rNote: \n",
    "                        \\r -The question must not have any text before or after.\n",
    "                        \\r -The question must less than 100 charactors. Wrapped the question within **.\"\"\"\n",
    "                       \n",
    "            if obs.step == 0:\n",
    "                prompts += \"\"\"\"BEGIN EXAMPLE\n",
    "                            \\r- Is it a country in Europe?\n",
    "                            \\r- Is it a country where its population more than 50 millions?\n",
    "                            \\r- Is it a city which start with 'L'?\n",
    "                            \\rEND EXAMPLE\"\"\"\n",
    "            else:\n",
    "                prompts += f\"\"\"\\r-{self._information}\"\"\"\n",
    "                return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "\n",
    "class Answerer(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answere reply only \"yes or \"no\". The keyword is the place. \n",
    "        You are the answerer, Yes or no if the question: \\\"{obs.questions[-1]}\\\", is the right answer for \\\"{obs.keyword}\\\".\n",
    "        \\rNote:\n",
    "        \\r -Answer only yes or no.\n",
    "        \\r -Example: **no**.\n",
    "        \\r -Wrapped the answer within **.\n",
    "        \"\"\"\n",
    "        \n",
    "        return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "############################################################\n",
    "agent = None\n",
    "\n",
    "# VARIANT = \"7b-it-quant\"\n",
    "VARIANT = \"2b-it\"\n",
    "# DEVICE = \"cuda\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "DEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_agent(name):\n",
    "    global agent\n",
    "\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Questioner(variant=VARIANT, device_str=DEVICE)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Answerer(variant=VARIANT, device_str=DEVICE)\n",
    "\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    selected_agent = None\n",
    "    if obs.turnType == \"ask\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    elif obs.turnType == \"guess\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    else:\n",
    "        selected_agent = get_agent(\"answerer\")\n",
    "        \n",
    "    try:\n",
    "        response = selected_agent.call(obs)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        response = \"no\"\n",
    "\n",
    "    if obs.turnType == \"answer\":\n",
    "        response = response.replace(\".\", \"\")\n",
    "        if response not in [\"yes\", \"no\"]: \n",
    "            response = \"no\"\n",
    "\n",
    "    if response is None or len(response) <= 1: return \"no\" \n",
    "    else: return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06147c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:48.936438Z",
     "iopub.status.busy": "2024-06-16T16:07:48.936151Z",
     "iopub.status.idle": "2024-06-16T16:07:48.940196Z",
     "shell.execute_reply": "2024-06-16T16:07:48.939439Z"
    },
    "papermill": {
     "duration": 0.009892,
     "end_time": "2024-06-16T16:07:48.942033",
     "exception": false,
     "start_time": "2024-06-16T16:07:48.932141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %run submission/main.py\n",
    "\n",
    "# def agent_fn(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# debug_config = {'episodeSteps':21,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 120,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True, configuration=debug_config)\n",
    "# agent = \"/kaggle/working/submission/main.py\"\n",
    "# env.reset()\n",
    "# logs = env.run([agent, agent_fn, agent_fn, agent_fn])\n",
    "# #while not env.done: #add steps here for testing\n",
    "# env.render(mode=\"ipython\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4f5465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:48.949830Z",
     "iopub.status.busy": "2024-06-16T16:07:48.949573Z",
     "iopub.status.idle": "2024-06-16T16:07:48.953751Z",
     "shell.execute_reply": "2024-06-16T16:07:48.952969Z"
    },
    "papermill": {
     "duration": 0.010291,
     "end_time": "2024-06-16T16:07:48.955650",
     "exception": false,
     "start_time": "2024-06-16T16:07:48.945359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# # For debugging, play game with only two rounds\n",
    "# debug_config = {'episodeSteps': 31,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 60,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "\n",
    "# env = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "# print(\"start.....\")\n",
    "# game_output = env.run(agents=[agent_fn, agent_fn, agent_fn, agent_fn])\n",
    "# print(\"finish....\")\n",
    "# env.render(mode=\"ipython\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8851e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:48.963407Z",
     "iopub.status.busy": "2024-06-16T16:07:48.963120Z",
     "iopub.status.idle": "2024-06-16T16:07:55.801956Z",
     "shell.execute_reply": "2024-06-16T16:07:55.800993Z"
    },
    "papermill": {
     "duration": 6.845178,
     "end_time": "2024-06-16T16:07:55.804206",
     "exception": false,
     "start_time": "2024-06-16T16:07:48.959028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b745426f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-16T16:07:55.813610Z",
     "iopub.status.busy": "2024-06-16T16:07:55.812847Z",
     "iopub.status.idle": "2024-06-16T16:10:13.921859Z",
     "shell.execute_reply": "2024-06-16T16:10:13.920829Z"
    },
    "papermill": {
     "duration": 138.116274,
     "end_time": "2024-06-16T16:10:13.924288",
     "exception": false,
     "start_time": "2024-06-16T16:07:55.808014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.745071,
   "end_time": "2024-06-16T16:10:14.157511",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-16T16:07:31.412440",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
