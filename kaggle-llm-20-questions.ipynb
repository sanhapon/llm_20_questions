{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb822bf",
   "metadata": {
    "papermill": {
     "duration": 0.0038,
     "end_time": "2024-06-19T04:31:06.425284",
     "exception": false,
     "start_time": "2024-06-19T04:31:06.421484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee7d5d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:06.432891Z",
     "iopub.status.busy": "2024-06-19T04:31:06.432529Z",
     "iopub.status.idle": "2024-06-19T04:31:19.672295Z",
     "shell.execute_reply": "2024-06-19T04:31:19.671434Z"
    },
    "papermill": {
     "duration": 13.245897,
     "end_time": "2024-06-19T04:31:19.674291",
     "exception": false,
     "start_time": "2024-06-19T04:31:06.428394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv -n /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "echo \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ce9cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:19.682224Z",
     "iopub.status.busy": "2024-06-19T04:31:19.681950Z",
     "iopub.status.idle": "2024-06-19T04:31:19.687811Z",
     "shell.execute_reply": "2024-06-19T04:31:19.686944Z"
    },
    "papermill": {
     "duration": 0.01202,
     "end_time": "2024-06-19T04:31:19.689654",
     "exception": false,
     "start_time": "2024-06-19T04:31:19.677634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import gc\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/working/submission/lib/\")\n",
    "# WEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# import contextlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# import torch\n",
    "# from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "# from gemma.model import GemmaForCausalLM\n",
    "# import re\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# # Load the model           \n",
    "# @contextlib.contextmanager\n",
    "# def _set_default_tensor_type(dtype: torch.dtype):\n",
    "#   torch.set_default_dtype(dtype)\n",
    "#   yield\n",
    "#   torch.set_default_dtype(torch.float)\n",
    "    \n",
    "\n",
    "# model_config =  get_config_for_7b()\n",
    "# model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "# model_config.quant = True\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "# model = None\n",
    "# gc.collect()\n",
    "\n",
    "# print(f\"create model... {model_config}\\r\\n\")\n",
    "# with _set_default_tensor_type(model_config.get_dtype()):\n",
    "#     model = GemmaForCausalLM(model_config)\n",
    "#     ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-7b-it-quant.ckpt')\n",
    "#     model.load_weights(ckpt_path)\n",
    "#     model = model.to(device).eval()\n",
    "\n",
    "    \n",
    "# def get_response(prompts):\n",
    "#     CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}.<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "#     sampler_kwargs = {\n",
    "#         'temperature': 0.01,\n",
    "#         'top_p': 0.1,\n",
    "#         'top_k': 1,\n",
    "#     }\n",
    "#     resp = model.generate(\n",
    "#                     CHAT_TEMPLATE.format(prompts=prompts),\n",
    "#                     device=device,\n",
    "#                     output_len=100,\n",
    "#                     **sampler_kwargs\n",
    "#     )\n",
    "\n",
    "#     return resp\n",
    "\n",
    "# prompts = [\n",
    "#         \"\"\"We are playing the 20 Questions game. The questioner asks the yes-or-no-question and guesses to identify the secret word . The keyword is the place. The answerer replies yes-or-no only.\n",
    "# Ask yes-or-no question without conversational sentence. Without information, your question must check is this place located in which continent.\n",
    "# Your generated text must less than 100 charactors and wrapped within **.\"\"\",\n",
    "    \n",
    "#     \"\"\"We are playing the 20 Questions game. The questioner asks the yes-or-no-question and guesses to identify the secret word . The keyword is the place. The answerer replies yes-or-no only.\n",
    "#           Based on this information: \\\"It is in the North America continent\\\"\n",
    "#           As a questioner, generate yes-no question to narrow down to city or area.\n",
    "#           The text must less than 30 charactors and wrapped within **.\"\"\"\n",
    "# ]\n",
    "\n",
    "# for i, p in enumerate(prompts):\n",
    "#     resp = get_response(p)\n",
    "#     print(f\"\"\"{i}: {p} \\r\\n{resp}\\r\\n------------------------------\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac02e2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:19.697337Z",
     "iopub.status.busy": "2024-06-19T04:31:19.697053Z",
     "iopub.status.idle": "2024-06-19T04:31:19.707324Z",
     "shell.execute_reply": "2024-06-19T04:31:19.706487Z"
    },
    "papermill": {
     "duration": 0.016337,
     "end_time": "2024-06-19T04:31:19.709171",
     "exception": false,
     "start_time": "2024-06-19T04:31:19.692834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/main.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    \n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "import re\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Load the model           \n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "    \n",
    "### Utils function\n",
    "def extract_bold_text(text):\n",
    "  pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n",
    "  matches = re.findall(pattern, text)\n",
    "  to_ret = matches[0] if len(matches) > 0 else text\n",
    "  parts = to_ret.split(\":\") \n",
    "  to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n",
    "  return to_ret\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, variant, device_str):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device_str)\n",
    "        self._weigts_path = WEIGHTS_PATH\n",
    "\n",
    "        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "          self._model = GemmaForCausalLM(model_config)\n",
    "          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n",
    "          self._model.load_weights(ckpt_path)\n",
    "          self._model = self._model.to(self._device).eval()\n",
    "            \n",
    "\n",
    "    def get_response_from_llm(self, obs, text): \n",
    "        CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        prompts = CHAT_TEMPLATE.format(prompts=text)\n",
    "        \n",
    "        try:\n",
    "            resp = self._model.generate(prompts, device=self._device, output_len=150, **sampler_kwargs)\n",
    "            print(f\"\"\"prompts: {prompts}\\nresp: {resp}\"\"\")\n",
    "        except RuntimeError as e:\n",
    "            resp = \"**no**\"\n",
    "        \n",
    "        resp = resp.replace(\"**Sure, here's the question:**\", \"\")\n",
    "        resp = resp.replace(\"Sure, here's the conversion:\", \"\")\n",
    "        resp = resp.replace(\"Sure, \", \"\")\n",
    "        resp = resp.replace(\"here's the \", \"\")\n",
    "        resp = resp.replace(\"question:\", \"\")\n",
    "        resp = resp.replace(\"conversion:\", \"\")\n",
    "        resp = resp.replace(\"**Guess:**\", \"\")\n",
    "        resp = resp.replace(\"**Anwser:**\", \"\")\n",
    "        resp = resp.replace(\"**Question:**\", \"\")\n",
    "\n",
    "        return extract_bold_text(resp)\n",
    "\n",
    "class Questioner(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = \"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answerer reply only \"yes or \"no\". The keyword is the place.\"\"\"\n",
    "\n",
    "        if obs.turnType == 'guess':\n",
    "            if obs.step > 0:\n",
    "                self._information = super().get_response_from_llm(obs,\n",
    "                    f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\". Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\"\"\"\n",
    "                )\n",
    "            prompts += f\"\"\"\n",
    "Now, you are the guesser. Guess for the keyword.\n",
    "Note:\n",
    "- The guess \"MUST\" be the keyword only. DO NOT provider the reason.\n",
    "- Example \"**Grand Palace**\".\n",
    "- The guess must less than 100 charactors. Wrapped the question within **.\n",
    "- {self._information}\n",
    "\"\"\"\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "\n",
    "        elif obs.turnType == 'ask':\n",
    "            prompts += f\"\"\"\n",
    "Now, you are the questioner. Ask yes-or-no question without conversational sentence. Your question must check and narrow the place is located in somewhere.\n",
    "Note:\n",
    "-The question must not have any text before or after.\n",
    "-The question must less than 100 charactors. Wrapped the question within **.\n",
    "\"\"\"\n",
    "                       \n",
    "            if obs.step == 0:\n",
    "                prompts += \"\"\"\n",
    "BEGIN EXAMPLE\n",
    " - Is it a country in Europe?\n",
    " - Is it a city which start with 'L'?\n",
    "\"\"\"\n",
    "            else:\n",
    "                prompts += f\"\"\"-{self._information}\"\"\"\n",
    "            \n",
    "            prompts += \"\"\" END EXAMPLE\"\"\"\n",
    "\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "\n",
    "class Answerer(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answerer reply only \"yes or \"no\". The keyword is the place. \n",
    "        You are the answerer, Yes or no if the question: \\\"{obs.questions[-1]}\\\", is the right answer for \\\"{obs.keyword}\\\".\n",
    "        Note:\n",
    " -Answer only yes or no.\n",
    " -Example: no\n",
    " -Wrapped the answer within **.\"\"\"\n",
    "        \n",
    "        response =  super().get_response_from_llm(obs, prompts)\n",
    "       \n",
    "        if response == None:\n",
    "            response = \"no\"\n",
    "\n",
    "        response = response.replace(\".\", \"\")\n",
    "        if response not in [\"yes\", \"no\"]: \n",
    "            response = \"no\"\n",
    "        \n",
    "        return response\n",
    "                \n",
    "############################################################\n",
    "agent = None\n",
    "\n",
    "# VARIANT = \"7b-it-quant\"\n",
    "VARIANT = \"2b-it\"\n",
    "# DEVICE = \"cuda\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "DEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_agent(name):\n",
    "    global agent\n",
    "\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Questioner(variant=VARIANT, device_str=DEVICE)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Answerer(variant=VARIANT, device_str=DEVICE)\n",
    "\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    print(f\"\"\"--obsinfo: {obs}\"\"\")\n",
    "    selected_agent = None\n",
    "    if obs.turnType == \"ask\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    elif obs.turnType == \"guess\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    else:\n",
    "        selected_agent = get_agent(\"answerer\")\n",
    "        \n",
    "    try:\n",
    "        response = selected_agent.call(obs)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        response = \"no\"\n",
    "\n",
    "    if response is None or len(response) <= 1: return \"no\" \n",
    "    else: return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a7b356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:19.716911Z",
     "iopub.status.busy": "2024-06-19T04:31:19.716597Z",
     "iopub.status.idle": "2024-06-19T04:31:19.720929Z",
     "shell.execute_reply": "2024-06-19T04:31:19.720119Z"
    },
    "papermill": {
     "duration": 0.010219,
     "end_time": "2024-06-19T04:31:19.722785",
     "exception": false,
     "start_time": "2024-06-19T04:31:19.712566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %run submission/main.py\n",
    "\n",
    "# def agent_fn(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# debug_config = {'episodeSteps':20,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 120,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "# from kaggle_environments import make\n",
    "# # env = make(\"llm_20_questions\", debug=True, configuration=debug_config)\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# agent = \"/kaggle/working/submission/main.py\"\n",
    "# env.reset()\n",
    "# logs = env.run([agent, agent, agent_fn, agent_fn])\n",
    "# #while not env.done: #add steps here for testing\n",
    "# env.render(mode=\"ipython\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73f868d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:19.730636Z",
     "iopub.status.busy": "2024-06-19T04:31:19.730377Z",
     "iopub.status.idle": "2024-06-19T04:31:19.734849Z",
     "shell.execute_reply": "2024-06-19T04:31:19.733895Z"
    },
    "papermill": {
     "duration": 0.010584,
     "end_time": "2024-06-19T04:31:19.736721",
     "exception": false,
     "start_time": "2024-06-19T04:31:19.726137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# # For debugging, play game with only two rounds\n",
    "# debug_config = {'episodeSteps': 31,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 60,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "\n",
    "# env = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "# print(\"start.....\")\n",
    "# game_output = env.run(agents=[agent_fn, agent_fn, agent_fn, agent_fn])\n",
    "# print(\"finish....\")\n",
    "# env.render(mode=\"ipython\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8041c14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:19.744554Z",
     "iopub.status.busy": "2024-06-19T04:31:19.744286Z",
     "iopub.status.idle": "2024-06-19T04:31:25.872472Z",
     "shell.execute_reply": "2024-06-19T04:31:25.871473Z"
    },
    "papermill": {
     "duration": 6.134634,
     "end_time": "2024-06-19T04:31:25.874766",
     "exception": false,
     "start_time": "2024-06-19T04:31:19.740132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3be4662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-19T04:31:25.883783Z",
     "iopub.status.busy": "2024-06-19T04:31:25.883441Z",
     "iopub.status.idle": "2024-06-19T04:33:39.420727Z",
     "shell.execute_reply": "2024-06-19T04:33:39.419810Z"
    },
    "papermill": {
     "duration": 133.5444,
     "end_time": "2024-06-19T04:33:39.422995",
     "exception": false,
     "start_time": "2024-06-19T04:31:25.878595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 156.060386,
   "end_time": "2024-06-19T04:33:39.755234",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-19T04:31:03.694848",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
