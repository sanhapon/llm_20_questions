{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079b2604",
   "metadata": {
    "papermill": {
     "duration": 0.00361,
     "end_time": "2024-06-17T08:40:03.634543",
     "exception": false,
     "start_time": "2024-06-17T08:40:03.630933",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f44338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:03.642112Z",
     "iopub.status.busy": "2024-06-17T08:40:03.641758Z",
     "iopub.status.idle": "2024-06-17T08:40:16.831679Z",
     "shell.execute_reply": "2024-06-17T08:40:16.830753Z"
    },
    "papermill": {
     "duration": 13.196556,
     "end_time": "2024-06-17T08:40:16.834094",
     "exception": false,
     "start_time": "2024-06-17T08:40:03.637538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'gemma_pytorch'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv -n /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "echo \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23b664f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:16.842488Z",
     "iopub.status.busy": "2024-06-17T08:40:16.842175Z",
     "iopub.status.idle": "2024-06-17T08:40:16.852282Z",
     "shell.execute_reply": "2024-06-17T08:40:16.851430Z"
    },
    "papermill": {
     "duration": 0.017006,
     "end_time": "2024-06-17T08:40:16.854465",
     "exception": false,
     "start_time": "2024-06-17T08:40:16.837459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile submission/main.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    \n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "import re\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# Load the model           \n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "  torch.set_default_dtype(dtype)\n",
    "  yield\n",
    "  torch.set_default_dtype(torch.float)\n",
    "    \n",
    "### Utils function\n",
    "def extract_bold_text(text):\n",
    "  pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n",
    "  matches = re.findall(pattern, text)\n",
    "  to_ret = matches[0] if len(matches) > 0 else text\n",
    "  parts = to_ret.split(\":\") \n",
    "  to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n",
    "  return to_ret\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, variant, device_str):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device_str)\n",
    "        self._weigts_path = WEIGHTS_PATH\n",
    "\n",
    "        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "          self._model = GemmaForCausalLM(model_config)\n",
    "          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n",
    "          self._model.load_weights(ckpt_path)\n",
    "          self._model = self._model.to(self._device).eval()\n",
    "            \n",
    "\n",
    "    def get_response_from_llm(self, obs, text): \n",
    "        CHAT_TEMPLATE = \"<start_of_turn>user\\n{prompts}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        prompts = CHAT_TEMPLATE.format(prompts=text)\n",
    "        \n",
    "        try:\n",
    "            resp = self._model.generate(prompts, device=self._device, output_len=200, **sampler_kwargs)\n",
    "            print(f\"\"\"prompts: {prompts}\\nresp: {resp}\"\"\")\n",
    "        except RuntimeError as e:\n",
    "            resp = \"**no**\"\n",
    "        \n",
    "        resp = resp.replace(\"**Sure, here's the question:**\", \"\")\n",
    "        resp = resp.replace(\"Sure, here's the conversion:\", \"\")\n",
    "        resp = resp.replace(\"Sure, \", \"\")\n",
    "        resp = resp.replace(\"here's the \", \"\")\n",
    "        resp = resp.replace(\"question:\", \"\")\n",
    "        resp = resp.replace(\"conversion:\", \"\")\n",
    "        resp = resp.replace(\"**Guess:**\", \"\")\n",
    "        resp = resp.replace(\"**Anwser:**\", \"\")\n",
    "        resp = resp.replace(\"**Question:**\", \"\")\n",
    "\n",
    "        return extract_bold_text(resp)\n",
    "\n",
    "class Questioner(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answere reply only \"yes or \"no\". The keyword is the place.\"\"\"\n",
    "\n",
    "        if obs.turnType == 'guess':\n",
    "            if obs.step > 0:\n",
    "                self._information = super().get_response_from_llm(obs,\n",
    "                    f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\". Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\"\"\"\n",
    "                )\n",
    "            prompts += f\"\"\"Now, you are the guesser. Guess for the keyword.\n",
    "                        \\rNote:\n",
    "                        \\r- The guess \"MUST\" be the keyword only. DO NOT provider the reason.\n",
    "                        \\r- Example \"**Grand Palace**\".\n",
    "                        \\r- The guess must less than 100 charactors. Wrapped the question within **.\n",
    "                        \\r - {self._information}\n",
    "                        \"\"\"\n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "\n",
    "        elif obs.turnType == 'ask':\n",
    "            prompts += f\"\"\"Now, you are the questioner. Ask yes-or-no question without conversational sentence. Your question must check and narrow the place is located in somewhere.\n",
    "                        \\rNote: \n",
    "                        \\r -The question must not have any text before or after.\n",
    "                        \\r -The question must less than 100 charactors. Wrapped the question within **.\"\"\"\n",
    "                       \n",
    "            if obs.step == 0:\n",
    "                prompts += \"\"\"\"BEGIN EXAMPLE\n",
    "                            \\r- Is it a country in Europe?\n",
    "                            \\r- Is it a country where its population more than 50 millions?\n",
    "                            \\r- Is it a city which start with 'L'?\n",
    "                            \\rEND EXAMPLE\"\"\"\n",
    "            else:\n",
    "                prompts += f\"\"\"\\r-{self._information}\"\"\"\n",
    "            \n",
    "            return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "\n",
    "class Answerer(BaseAgent):\n",
    "    def __init__(self, variant, device_str):\n",
    "        super().__init__(variant, device_str)\n",
    "        self._information = \"\"\n",
    "    \n",
    "    def call(self, obs):\n",
    "        prompts = f\"\"\"You are a helpful AI assistant in the 20 Questions game. The questioner \"asks\" the yes-or-no-question and \"guesses\" to identify the secret word. The answere reply only \"yes or \"no\". The keyword is the place. \n",
    "        You are the answerer, Yes or no if the question: \\\"{obs.questions[-1]}\\\", is the right answer for \\\"{obs.keyword}\\\".\n",
    "        \\rNote:\n",
    "        \\r -Answer only yes or no.\n",
    "        \\r -Example: **no**.\n",
    "        \\r -Wrapped the answer within **.\n",
    "        \"\"\"\n",
    "        \n",
    "        return super().get_response_from_llm(obs, prompts)\n",
    "                \n",
    "############################################################\n",
    "agent = None\n",
    "\n",
    "VARIANT = \"7b-it-quant\"\n",
    "# VARIANT = \"2b-it\"\n",
    "# DEVICE = \"cuda\"\n",
    "# DEVICE = \"cpu\"\n",
    "\n",
    "DEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_agent(name):\n",
    "    global agent\n",
    "\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Questioner(variant=VARIANT, device_str=DEVICE)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Answerer(variant=VARIANT, device_str=DEVICE)\n",
    "\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    print(f\"\"\"--obsinfo: {obs}\"\"\")\n",
    "    selected_agent = None\n",
    "    if obs.turnType == \"ask\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    elif obs.turnType == \"guess\":\n",
    "        selected_agent = get_agent(\"questioner\")\n",
    "    else:\n",
    "        selected_agent = get_agent(\"answerer\")\n",
    "        \n",
    "    try:\n",
    "        response = selected_agent.call(obs)\n",
    "    except Exception as e:\n",
    "        print(f\"error: {e}\")\n",
    "        response = \"no\"\n",
    "\n",
    "    if obs.turnType == \"answer\":\n",
    "        response = response.replace(\".\", \"\")\n",
    "        if response not in [\"yes\", \"no\"]: \n",
    "            response = \"no\"\n",
    "\n",
    "    if response is None or len(response) <= 1: return \"no\" \n",
    "    else: return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4412fcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:16.861916Z",
     "iopub.status.busy": "2024-06-17T08:40:16.861363Z",
     "iopub.status.idle": "2024-06-17T08:40:20.441673Z",
     "shell.execute_reply": "2024-06-17T08:40:20.440670Z"
    },
    "papermill": {
     "duration": 3.586637,
     "end_time": "2024-06-17T08:40:20.444172",
     "exception": false,
     "start_time": "2024-06-17T08:40:16.857535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run submission/main.py\n",
    "\n",
    "# def agent_fn(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# debug_config = {'episodeSteps':10,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 120,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True, configuration=debug_config)\n",
    "# agent = \"/kaggle/working/submission/main.py\"\n",
    "# env.reset()\n",
    "# logs = env.run([agent, agent_fn, agent_fn, agent_fn])\n",
    "# #while not env.done: #add steps here for testing\n",
    "# env.render(mode=\"ipython\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddcdedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:20.451999Z",
     "iopub.status.busy": "2024-06-17T08:40:20.451638Z",
     "iopub.status.idle": "2024-06-17T08:40:20.456205Z",
     "shell.execute_reply": "2024-06-17T08:40:20.455354Z"
    },
    "papermill": {
     "duration": 0.010615,
     "end_time": "2024-06-17T08:40:20.458112",
     "exception": false,
     "start_time": "2024-06-17T08:40:20.447497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "#     elif obs.turnType == \"guess\": response = \"pig\"\n",
    "#     elif obs.turnType == \"answer\": response = \"yes\"\n",
    "#     return response\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# # For debugging, play game with only two rounds\n",
    "# debug_config = {'episodeSteps': 31,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "#                 'actTimeout': 60,       # agent time per round in seconds; default is 60\n",
    "#                 'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n",
    "#                 'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "\n",
    "# env = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "# print(\"start.....\")\n",
    "# game_output = env.run(agents=[agent_fn, agent_fn, agent_fn, agent_fn])\n",
    "# print(\"finish....\")\n",
    "# env.render(mode=\"ipython\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d394999a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:20.465327Z",
     "iopub.status.busy": "2024-06-17T08:40:20.465051Z",
     "iopub.status.idle": "2024-06-17T08:40:26.119406Z",
     "shell.execute_reply": "2024-06-17T08:40:26.118233Z"
    },
    "papermill": {
     "duration": 5.660573,
     "end_time": "2024-06-17T08:40:26.121782",
     "exception": false,
     "start_time": "2024-06-17T08:40:20.461209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d150ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T08:40:26.130051Z",
     "iopub.status.busy": "2024-06-17T08:40:26.129730Z",
     "iopub.status.idle": "2024-06-17T08:42:51.069649Z",
     "shell.execute_reply": "2024-06-17T08:42:51.068633Z"
    },
    "papermill": {
     "duration": 144.94657,
     "end_time": "2024-06-17T08:42:51.071952",
     "exception": false,
     "start_time": "2024-06-17T08:40:26.125382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 171.301892,
   "end_time": "2024-06-17T08:42:52.208314",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-17T08:40:00.906422",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
