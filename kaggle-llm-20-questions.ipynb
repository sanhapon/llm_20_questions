{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61247,"databundleVersionId":8550470,"sourceType":"competition"},{"sourceId":11358,"sourceType":"modelInstanceVersion","modelInstanceId":5383},{"sourceId":11359,"sourceType":"modelInstanceVersion","modelInstanceId":8749}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":169.923583,"end_time":"2024-04-17T13:50:23.369773","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-17T13:47:33.44619","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sanhapon/test-fork-of-llm-20-questions-simple-version?scriptVersionId=183575935\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. ","metadata":{}},{"cell_type":"code","source":"%%bash\ncd /kaggle/working\npip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\ngit clone https://github.com/google/gemma_pytorch.git > /dev/null\nmkdir /kaggle/working/submission/lib/gemma/\nmv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/","metadata":{"papermill":{"duration":13.257308,"end_time":"2024-04-17T13:47:49.310664","exception":false,"start_time":"2024-04-17T13:47:36.053356","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%writefile submission/main.py?\n# Setup\nimport os\nimport sys\n\n# **IMPORTANT:** Set up your system path like this to make your code work\n# both in notebooks and in the simulations environment.\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nif os.path.exists(KAGGLE_AGENT_PATH):\n    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n#     WEIGHTS_PATH  = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\nelse:\n    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n#     WEIGHTS_PATH  = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n\n\nimport contextlib\nfrom pathlib import Path\n\nimport torch\nfrom gemma.config import get_config_for_7b, get_config_for_2b\nfrom gemma.model import GemmaForCausalLM\nimport re\n\n###Constant\nsampler_kwargs = {\n        'temperature': 0.01,\n        'top_p': 0.1,\n        'top_k': 1,\n}\n\n# Load the model           \n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n    \n\nclass BaseAgent:\n    def __init__(self, variant, device_str):\n        self._variant = variant\n        self._device = torch.device(device_str)\n        self._weigts_path = WEIGHTS_PATH\n\n        model_config = get_config_for_2b() if \"2b\" in self._variant else get_config_for_7b()\n        model_config.tokenizer = os.path.join(self._weigts_path, \"tokenizer.model\")\n        model_config.quant = \"quant\" in variant\n\n        print(f\"load model\")\n        with _set_default_tensor_type(model_config.get_dtype()):\n          self._model = GemmaForCausalLM(model_config)\n          ckpt_path = os.path.join(self._weigts_path, f'gemma-{variant}.ckpt')\n          self._model.load_weights(ckpt_path)\n          self._model = self._model.to(self._device).eval()\n\n    def extract_bold_text(self, text):\n      pattern = r\"[\\*|\\\"]+(.*?)[\\*|\\\"]+\"\n      matches = re.findall(pattern, text)\n      to_ret = matches[0] if len(matches) > 0 else text\n      parts = to_ret.split(\":\") \n      to_ret = parts[1].strip() if len(parts) > 1 else to_ret\n      return to_ret\n\n    def get_response_from_llm(self, obs, prompts): \n        print(prompts)\n        try:\n            resp = self._model.generate(prompts, device=self._device, output_len=100, **sampler_kwargs)\n            print(f\"{obs.step}: {resp}\")\n        except RuntimeError as e:\n            print(f\"Error in LLM generation: {e}\")\n            resp = \"**no**\"\n            resp = resp.replace(\"**Question:**\", \"\").replace(\"**Guess:**\", \"\").replace(\"**Anwser:**\", \"\")\n        return extract_bold_text(resp)\n\nclass Answerer(BaseAgent):\n    def __init__(self, variant, device_str):\n        super().__init__(variant, device_str)\n        self._information = \"\"\n    \n    def call(self, obs):\n        prompts = f\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name.\n                    If this question \\\"{obs.questions[-1]}\\\" relates to \\\"{obs.keyword}\\\" return \\\"yes\\\" otherwise return \\\"no\\\".\n                    \"\"\"\n        return super().get_response_from_llm(obs, prompts)\n                \n\nclass Questioner(BaseAgent):\n    def __init__(self, variant, device_str):\n        super().__init__(variant, device_str)\n        self._information = \"\"\n    \n    def call(self, obs):\n        prompts = f\"\"\"We are playing the 20 Questions game. The questioner's goal is to ask the question and guess for secret. The secret in the game is the country or city name.\"\"\"\n        if obs.turnType == 'guess':\n            prompts = f\"\"\"Based on this information \\\"{self._information}\\\".\n                            Guess the place based on above information.\"\"\"\n\n            return super().get_response_from_llm(obs, prompts)\n\n        elif obs.turnType == 'ask':\n            if obs.step == 0:\n                prompts += f\"\"\"The sample question: Is it a Asia continent? \n                            Now, you are the questioner, generate a yes-or-no question to find the place. Enclose your question in *.\n                         \"\"\"\n                return super().get_response_from_llm(obs, prompts)\n            else:\n                self._information = super().get_response_from_llm(obs,\n                    f\"\"\"Convert the yes-or-no question to declarative sentence, for example: \"Is it a cat?\", \"Yes\". The declarative sentence would be \"It is a cat\".\n                        Here is the yes-or-no question from previous round: \\\"{obs.questions[-1]}\\\",  \\\"{obs.answers[-1]}\\\".\n                        Enclose your declarative sentence with *.\"\"\")\n\n                prompts = f\"\"\"We know this information \\\"{self._information}\\\".\n                            Now, you are the questioner, based on known information, generate yes-or-no question to find \"specific\" place. Enclose your question with *.\"\"\"\n\n                return super().get_response_from_llm(obs, prompts)\n                \n\n\n############################################################\nagent = None\n\n# VARIANT = \"7b-it-quant\"\nVARIANT = \"2b-it\"\n# DEVICE = \"cuda:0\"\n# DEVICE = \"cpu\"\n\nDEVICE =  \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ndef agent_fn(obs, cfg):\n    global agent\n    \n    print(f\"{obs.turnType}: call\")\n    \n    if agent is None:\n        print(f\"Create anwserer\")\n        if obs.turnType == \"answer\":\n            agent = Answerer(variant=VARIANT, device_str=DEVICE)        \n        else:\n            print(f\"Create questioner\")\n            agent = Questioner(variant=VARIANT, device_str=DEVICE)\n    \n    try:\n        response = agent.call(obs)\n    except Exception as e:\n        print(f\"error: {e}\")\n        response = \"no\"\n    \n\n    if obs.turnType == \"answer\":\n        response = response.replace(\".\", \"\")\n        if response not in [\"yes\", \"no\"]: \n            response = \"no\"\n\n    if response is None or len(response) <= 1: return \"no\" \n    else: return response","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def simple_agent(obs, cfg):\n    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n    elif obs.turnType == \"guess\": response = \"pig\"\n    elif obs.turnType == \"answer\": response = \"yes\"\n    return response\n\nfrom kaggle_environments import make\n# For debugging, play game with only two rounds\ndebug_config = {'episodeSteps': 13,     # initial step plus 3 steps per round (ask/answer/guess)\n                'actTimeout': 60,       # agent time per round in seconds; default is 60\n                'runTimeout': 1200,      # max time for the episode in seconds; default is 1200\n                'agentTimeout': 3600}  # obsolete field; default is 3600\n\nenv = make(\"llm_20_questions\", configuration=debug_config, debug=True)\n\nprint(\"start.....\")\ngame_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\nprint(\"finish....\")\nenv.render(mode=\"ipython\", width=700, height=700)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !apt install pigz pv > /dev/null","metadata":{"papermill":{"duration":5.560311,"end_time":"2024-04-17T13:47:54.892856","exception":false,"start_time":"2024-04-17T13:47:49.332545","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2","metadata":{"papermill":{"duration":148.240766,"end_time":"2024-04-17T13:50:23.136669","exception":false,"start_time":"2024-04-17T13:47:54.895903","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}